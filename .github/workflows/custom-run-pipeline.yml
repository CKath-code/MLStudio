name: run-pipeline

on:
  workflow_call:
    inputs:
      parameters-file:
        required: true
        type: string
      resource_group:
        required: true
        type: string
      workspace_name:
        required: true
        type: string
      job-name:
        required: true
        type: string
      dataset_name:
        required: true
        type: string
    secrets:
      AZURE_CLIENT_ID:
        required: true
      AZURE_TENANT_ID:
        required: true
      AZURE_SUBSCRIPTION_ID:
        required: true
      AZURE_CLIENT_SECRET:
        required: true

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: checkout
        uses: actions/checkout@v3
      - name: "Az CLI login"
        uses: azure/login@v1
        with:
          creds: ${{ format('{{"clientId":"{0}","clientSecret":"{1}","subscriptionId":"{2}","tenantId":"{3}"}}', secrets.AZURE_CLIENT_ID, secrets.AZURE_CLIENT_SECRET, secrets.AZURE_SUBSCRIPTION_ID, secrets.AZURE_TENANT_ID) }}
      - name: install-extension
        run: az extension add -n ml -y
      - name: update-extension
        run: az extension update -n ml
      - name: update-pipeline-with-dataset-name
        run: |
          # Debug: Show current directory and structure
          echo "Current working directory: $(pwd)"
          echo "Directory structure:"
          ls -la
          echo "data-science directory check:"
          ls -la data-science/src/ || echo "data-science/src not found"
          
          # Create a temporary pipeline file with the dynamic dataset name
          cp ${{ inputs.parameters-file }} temp-pipeline.yml
          # Replace the dataset reference with the dynamic name
          sed -i "s/azureml:used-cars-data[^@]*@latest/azureml:${{ inputs.dataset_name }}@latest/g" temp-pipeline.yml
          # Fix code paths to work in GitHub Actions environment  
          sed -i "s|code: \./data-science/src|code: ./data-science/src|g" temp-pipeline.yml
          echo "Updated pipeline file with dataset name: ${{ inputs.dataset_name }}"
          echo "Pipeline file content:"
          head -30 temp-pipeline.yml
      - name: run-ml-pipeline
        run: |
          run_id=$(az ml job create --file temp-pipeline.yml --resource-group ${{ inputs.resource_group }} --workspace-name ${{ inputs.workspace_name }} --query name -o tsv)
          if [[ -z "$run_id" ]]
          then
            echo "Job creation failed"
            exit 3
          fi
          az ml job show -n $run_id --resource-group ${{ inputs.resource_group }} --workspace-name ${{ inputs.workspace_name }} --web 
          status=$(az ml job show -n $run_id --resource-group ${{ inputs.resource_group }} --workspace-name ${{ inputs.workspace_name }} --query status -o tsv)
          if [[ -z "$status" ]]
          then
            echo "Status query failed"
            exit 4
          fi
          running=("NotStarted" "Queued" "Starting" "Preparing" "Running" "Finalizing" "CancelRequested")
          while [[ ${running[*]} =~ $status ]]
          do
            sleep 15 
            status=$(az ml job show -n $run_id --resource-group ${{ inputs.resource_group }} --workspace-name ${{ inputs.workspace_name }} --query status -o tsv)
            echo $status
          done
          if [[ "$status" != "Completed" ]]  
          then
            echo "Training Job failed or canceled"
            exit 3
          fi
